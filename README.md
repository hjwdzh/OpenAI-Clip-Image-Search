# OpenAI CLIP Text-to-Image Search

![image](https://github.com/jarvisx17/OpenAI-Clip-Image-Search/assets/91436985/3cc421c8-1187-488e-8fd6-f55f37d32a97)

This repository contains a Jupyter Notebook (`OpenAI_Clip_Faiss_Image_Search_.ipynb`) that demonstrates how to perform text-to-image search using the OpenAI CLIP model. CLIP (Contrastive Language-Image Pretraining) is a powerful model that can understand both text and images, making it suitable for cross-modal search tasks.

## Project Overview

The primary objective of this project is to show how to leverage the OpenAI CLIP model to search for images based on textual queries. It combines natural language understanding and computer vision to enable searching for images using descriptive text. 

### Notebook Contents

The notebook includes the following key components:

1. **Setup**: Instructions for setting up the environment, including installing required libraries and dependencies.

2. **Loading CLIP Model**: Loading the pre-trained CLIP model from OpenAI's resources.

3. **Data Preprocessing**: Preprocessing and preparing the image and text data for similarity computation.

4. **Image Search Functions**: Implementing functions to perform text-to-image search using CLIP and Faiss, an efficient similarity search library.

5. **Example Queries**: Demonstrating the search process with example textual queries.

## Getting Started

To explore this project and perform text-to-image searches using OpenAI CLIP, follow these steps:

1. Clone this repository to your local machine.

2. Open the Jupyter Notebook `OpenAI_Clip_Faiss_Image_Search_.ipynb` in your preferred Python environment.

3. Follow the instructions and code within the notebook to perform text-based image searches.

## Dependencies

This project relies on the following libraries and tools:

- Python 3.x
- Jupyter Notebook
- PyTorch
- OpenAI's CLIP model
- Faiss (a library for similarity search)
- PIL (Python Imaging Library) for image processing

You can install these dependencies using `pip` or `conda` as necessary.

## License

This project is licensed under the [MIT License](LICENSE).

## Acknowledgments

We would like to acknowledge OpenAI for developing the CLIP model and providing access to it. Their contributions have made this project possible.

---

Feel free to explore the provided Jupyter Notebook to learn how to perform text-to-image search using the OpenAI CLIP model. If you have any questions or encounter any issues, please don't hesitate to reach out to the project maintainers.
